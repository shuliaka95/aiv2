# train.py
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'ml'))

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR
from tqdm import tqdm
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from dataset import get_dataloaders
from model import ModulationNet25M as MyCustomModel
from config import TRAIN_CONFIG, MODEL_CONFIG, NUM_CLASSES, NUM_IQ_SAMPLES

class VerboseReduceLROnPlateau(ReduceLROnPlateau):
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π scheduler —Å –≤—ã–≤–æ–¥–æ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."""
    def __init__(self, optimizer, mode='min', factor=0.1, patience=10,
                 threshold=1e-4, threshold_mode='rel', cooldown=0,
                 min_lr=0, eps=1e-8):
        super().__init__(optimizer, mode, factor, patience, threshold,
                         threshold_mode, cooldown, min_lr, eps)
    
    def step(self, metrics):
        old_lr = self.optimizer.param_groups[0]['lr']
        super().step(metrics)
        new_lr = self.optimizer.param_groups[0]['lr']
        
        if new_lr != old_lr:
            print(f"  ‚Üª Learning rate –∏–∑–º–µ–Ω–µ–Ω: {old_lr:.2e} ‚Üí {new_lr:.2e}")

def train_model():
    """–ú–æ—â–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è."""
    device = TRAIN_CONFIG['device']
    epochs = TRAIN_CONFIG['num_epochs']
    model_save_path = MODEL_CONFIG['model_path']
    
    print(f"=== –û–ë–£–ß–ï–ù–ò–ï –ú–û–©–ù–û–ô –ú–û–î–ï–õ–ò –ù–ê {NUM_CLASSES} –ö–õ–ê–°–°–û–í ===")
    print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    print(f"–≠–ø–æ—Ö: {epochs}")
    print(f"Batch size: {TRAIN_CONFIG['batch_size']}")
    print(f"Learning rate: {TRAIN_CONFIG['learning_rate']}")
    print("=" * 70)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    train_loader, val_loader = get_dataloaders(
        batch_size=TRAIN_CONFIG['batch_size'],
        num_iq_samples=NUM_IQ_SAMPLES,
        num_classes=NUM_CLASSES,
    )
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    test_batch = next(iter(train_loader))
    print(f"–§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {test_batch[0].shape}")
    print(f"–î–∏–∞–ø–∞–∑–æ–Ω –º–µ—Ç–æ–∫: [{test_batch[1].min().item()}, {test_batch[1].max().item()}]")
    print(f"–í—Å–µ –º–µ—Ç–∫–∏ < {NUM_CLASSES}? {test_batch[1].max().item() < NUM_CLASSES}")
    
    # –î–ª—è IterableDataset –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤
    STEPS_PER_EPOCH = 15000  # –£–≤–µ–ª–∏—á–∏–ª –¥–ª—è –±–æ–ª—å—à–µ–π –º–æ–¥–µ–ª–∏
    VAL_STEPS = 1500

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    model = MyCustomModel(num_classes=NUM_CLASSES).to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\n=== –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ú–û–î–ï–õ–ò ===")
    print(f"–í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
    print(f"–û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {trainable_params:,}")
    print(f"–ü—Ä–æ–ø–æ—Ä—Ü–∏—è –æ–±—É—á–∞–µ–º—ã—Ö: {trainable_params/total_params:.2%}")
    
    # –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å —Å label smoothing
    criterion = nn.CrossEntropyLoss(label_smoothing=0.3)  # –£–≤–µ–ª–∏—á–∏–ª smoothing
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å –±–æ–ª—å—à–∏–º weight decay
    optimizer = optim.AdamW(model.parameters(),
                          lr=TRAIN_CONFIG['learning_rate'],
                          weight_decay=1e-4,
                          betas=(0.9, 0.999),
                          eps=1e-8)
    
    # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ schedulers
    #scheduler_plateau = VerboseReduceLROnPlateau(optimizer, mode='max', factor=0.5,
                                             #   patience=5, min_lr=1e-7)  # –ë–æ–ª—å—à–µ patience
    scheduler_cosine = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-7)
    
    # –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
    best_val_acc = 0.0
    best_val_loss = float('inf')
    patience_counter = 0
    max_patience = 12  # –£–≤–µ–ª–∏—á–∏–ª patience
    
    train_losses, train_accs = [], []
    val_losses, val_accs = [], []
    
    # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    train_iter = iter(train_loader)

    #  CURRICULUM LEARNING –ü–õ–ê–ù
    curriculum_phases = [
        {'name': '–§–∞–∑–∞ 1: –ú–Ω–æ–≥–æ —à—É–º–∞ (–±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)',
         'start_epoch': 0,
         'end_epoch': 15,
         'impairment': 1.0,
         'steps': 8000,
         'val_steps': 800},
         
        {'name': '–§–∞–∑–∞ 2: –°—Ä–µ–¥–Ω–∏–π —à—É–º (—Ç–æ–Ω–∫–∏–µ —Ä–∞–∑–ª–∏—á–∏—è)',
         'start_epoch': 16,
         'end_epoch': 35,
         'impairment': 2.0,
         'steps': 10000,
         'val_steps': 1000},
         
        {'name': '–§–∞–∑–∞ 3: –ú–∞–ª–æ —à—É–º–∞ (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å)',
         'start_epoch': 36,
         'end_epoch': 70,
         'impairment': 3.0,
         'steps': 12000,
         'val_steps': 1200}
    ]
    
    print("\n" + "="*70)
    print("üéØ CURRICULUM LEARNING –ü–õ–ê–ù:")
    print("="*70)
    for phase in curriculum_phases:
        print(f"{phase['name']}:")
        print(f"  –≠–ø–æ—Ö–∏: {phase['start_epoch']+1}-{phase['end_epoch']}")
        print(f"  –£—Ä–æ–≤–µ–Ω—å —à—É–º–∞: {phase['impairment']}")
        print(f"  –®–∞–≥–æ–≤/—ç–ø–æ—Ö—É: {phase['steps']:,}")
        print(f"  Val —à–∞–≥–æ–≤: {phase['val_steps']:,}")
        print("-" * 70)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∑–∞–≥—Ä—É–∑—á–∏–∫–∏ –¥–ª—è –ø–µ—Ä–≤–æ–π —Ñ–∞–∑—ã
    original_train_loader = train_loader
    original_val_loader = val_loader
    
    # ========== –û–°–ù–û–í–ù–û–ô –¶–ò–ö–õ –û–ë–£–ß–ï–ù–ò–Ø ==========
    best_val_acc = 0.0
    best_val_loss = float('inf')
    patience_counter = 0
    max_patience = 12
    
    train_losses, train_accs = [], []
    val_losses, val_accs = [], []
    
    current_phase_idx = 0
    current_phase = curriculum_phases[0]
    
    for epoch in range(epochs):
        # ===== –û–ü–†–ï–î–ï–õ–Ø–ï–ú –¢–ï–ö–£–©–£–Æ –§–ê–ó–£ CURRICULUM =====
        for i, phase in enumerate(curriculum_phases):
            if phase['start_epoch'] <= epoch <= phase['end_epoch']:
                if i != current_phase_idx:
                    current_phase_idx = i
                    current_phase = phase
                    
                    print(f"\n{'='*70}")
                    print(f"üöÄ –ü–ï–†–ï–•–û–î –ù–ê: {current_phase['name']}")
                    print(f"  –£—Ä–æ–≤–µ–Ω—å —à—É–º–∞: {current_phase['impairment']}")
                    print(f"  –®–∞–≥–æ–≤/—ç–ø–æ—Ö—É: {current_phase['steps']:,}")
                    print(f"{'='*70}")
                    
                    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
                    del train_loader, val_loader
                    torch.cuda.empty_cache()
                    
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ –∑–∞–≥—Ä—É–∑—á–∏–∫–∏ —Å –Ω–æ–≤—ã–º —É—Ä–æ–≤–Ω–µ–º —à—É–º–∞
                    train_loader, val_loader = get_dataloaders(
                        batch_size=TRAIN_CONFIG['batch_size'],
                        num_iq_samples=NUM_IQ_SAMPLES,
                        num_classes=NUM_CLASSES,
                        impairment_level=current_phase['impairment']
                    )
                break
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–µ–∫—É—â–µ–π —Ñ–∞–∑—ã
        phase_steps = current_phase['steps']
        phase_val_steps = current_phase['val_steps']
        
        # ===== –û–ë–£–ß–ï–ù–ò–ï =====
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        train_iter = iter(train_loader)
        progress_bar = tqdm(range(phase_steps),
                          desc=f"–≠–ø–æ—Ö–∞ {epoch+1}/{epochs} [{current_phase['name'][:15]}...]",
                          leave=False)
        
        for step in progress_bar:
            try:
                inputs, targets = next(train_iter)
            except StopIteration:
                train_iter = iter(train_loader)
                inputs, targets = next(train_iter)
            
            inputs, targets = inputs.to(device), targets.to(device)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –±–∞—Ç—á—É
            inputs = (inputs - inputs.mean(dim=(0, 2), keepdim=True)) / (inputs.std(dim=(0, 2), keepdim=True) + 1e-8)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.3)
            optimizer.step()
            
            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            batch_total = targets.size(0)
            total += batch_total
            correct += (predicted == targets).sum().item()
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
            if step % 50 == 0:
                avg_loss = running_loss / (step + 1)
                accuracy = 100 * correct / total if total > 0 else 0
                progress_bar.set_postfix(
                    loss=f"{avg_loss:.4f}", 
                    acc=f"{accuracy:.2f}%",
                    lr=f"{optimizer.param_groups[0]['lr']:.1e}"
                )
        
        epoch_loss = running_loss / phase_steps
        epoch_acc = 100 * correct / total if total > 0 else 0
        train_losses.append(epoch_loss)
        train_accs.append(epoch_acc)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ scheduler
        scheduler_cosine.step()
        
        # ===== –í–ê–õ–ò–î–ê–¶–ò–Ø =====
        model.eval()
        val_correct = 0
        val_total = 0
        val_running_loss = 0.0
        
        val_iter = iter(val_loader)
        with torch.no_grad():
            for step in range(phase_val_steps):
                try:
                    inputs, targets = next(val_iter)
                except StopIteration:
                    val_iter = iter(val_loader)
                    inputs, targets = next(val_iter)
                
                inputs, targets = inputs.to(device), targets.to(device)
                inputs = (inputs - inputs.mean(dim=(0, 2), keepdim=True)) / (inputs.std(dim=(0, 2), keepdim=True) + 1e-8)
                
                outputs = model(inputs)
                val_loss = criterion(outputs, targets)
                val_running_loss += val_loss.item()
                
                _, predicted = torch.max(outputs, 1)
                val_total += targets.size(0)
                val_correct += (predicted == targets).sum().item()
        
        val_loss = val_running_loss / phase_val_steps
        val_acc = 100 * val_correct / val_total if val_total > 0 else 0
        val_losses.append(val_loss)
        val_accs.append(val_acc)
        
        # ===== –í–´–í–û–î –°–¢–ê–¢–ò–°–¢–ò–ö–ò =====
        print(f"\n{'='*70}")
        print(f"–≠–ø–æ—Ö–∞ {epoch+1:3d}/{epochs} [{current_phase['name']}]")
        print(f"  –û–±—É—á–µ–Ω–∏–µ  - Loss: {epoch_loss:.4f}, Acc: {epoch_acc:6.2f}%")
        print(f"  –í–∞–ª–∏–¥–∞—Ü–∏—è - Loss: {val_loss:.4f}, Acc: {val_acc:6.2f}%")
        print(f"  LR: {optimizer.param_groups[0]['lr']:.2e}, Phase: {current_phase_idx+1}/3")
        print(f"  Gap: {epoch_acc - val_acc:.1f}%")
        
        # ===== –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò =====
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_val_loss = val_loss
            patience_counter = 0
            
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
                'val_loss': val_loss,
                'train_acc': epoch_acc,
                'train_loss': epoch_loss,
                'phase': current_phase_idx + 1,
                'impairment': current_phase['impairment']
            }, model_save_path)
            
            print(f"  üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (Acc: {val_acc:.2f}%, Phase: {current_phase_idx+1})")
        else:
            patience_counter += 1
            print(f"  ‚è≥ –ë–µ–∑ —É–ª—É—á—à–µ–Ω–∏–π: {patience_counter}/{max_patience}")
        
        # ===== –†–ê–ù–ù–Ø–Ø –û–°–¢–ê–ù–û–í–ö–ê =====
        if patience_counter >= max_patience:
            print(f"\n{'='*70}")
            print(f"‚ö†Ô∏è  –†–ê–ù–ù–Ø–Ø –û–°–¢–ê–ù–û–í–ö–ê –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
            print(f"   –§–∞–∑–∞: {current_phase['name']}")
            print(f"   –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_val_acc:.2f}%")
            break
        
        # ===== –ü–†–û–ì–†–ï–°–° –ö–ê–ñ–î–´–ï 10 –≠–ü–û–• =====
        if (epoch + 1) % 10 == 0:
            print(f"\n  üìä –ü—Ä–æ–≥—Ä–µ—Å—Å —á–µ—Ä–µ–∑ {epoch+1} —ç–ø–æ—Ö:")
            print(f"  –¢–µ–∫—É—â–∞—è —Ñ–∞–∑–∞: {current_phase['name']}")
            print(f"  –°—Ä–µ–¥–Ω–∏–π Loss (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 10): {np.mean(train_losses[-10:]):.4f}")
            print(f"  –°—Ä–µ–¥–Ω–∏–π Acc (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 10): {np.mean(train_accs[-10:]):.2f}%")
    
    # ===== –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê =====
    print(f"\n{'='*70}")
    print(f"üèÅ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    print(f"–í—Å–µ–≥–æ —ç–ø–æ—Ö: {len(train_accs)}")
    print(f"–õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_val_acc:.2f}%")
    print(f"–õ—É—á—à–∏–π Loss: {best_val_loss:.4f}")
    print(f"{'='*70}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞
    if os.path.exists(model_save_path):
        checkpoint = torch.load(model_save_path)
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"\nüì¶ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å:")
        print(f"  –≠–ø–æ—Ö–∞: {checkpoint['epoch']+1}")
        print(f"  –§–∞–∑–∞: {checkpoint['phase']}")
        print(f"  Accuracy: {checkpoint['val_acc']:.2f}%")
    # –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    if len(train_accs) > 10:
        final_gap = train_accs[-1] - val_accs[-1]
        avg_gap = np.mean([t - v for t, v in zip(train_accs[-10:], val_accs[-10:])])
        
        print(f"\n=== –ê–ù–ê–õ–ò–ó –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–Ø ===")
        print(f"–§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ (train-val): {final_gap:.2f}%")
        print(f"–°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑—Ä—ã–≤ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —ç–ø–æ—Ö): {avg_gap:.2f}%")
        
        if avg_gap > 15:
            print("  ‚ö†Ô∏è  –°–ò–õ–¨–ù–û–ï –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï!")
            print("  –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –£–≤–µ–ª–∏—á—å—Ç–µ dropout, —É–º–µ–Ω—å—à–∏—Ç–µ –º–æ–¥–µ–ª—å")
        elif avg_gap > 8:
            print("  ‚ö†Ô∏è  –£–ú–ï–†–ï–ù–ù–û–ï –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï")
        else:
            print("  ‚úì –•–û–†–û–®–ê–Ø –û–ë–û–ë–©–ê–Æ–©–ê–Ø –°–ü–û–°–û–ë–ù–û–°–¢–¨")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∞
    if os.path.exists(model_save_path):
        checkpoint = torch.load(model_save_path)
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"\n–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å –∏–∑ —ç–ø–æ—Ö–∏ {checkpoint['epoch']+1}")
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ç–µ—Å—Ç
        model.eval()
        all_predictions = []
        all_targets = []
        
        val_iter = iter(val_loader)
        with torch.no_grad():
            for _ in range(VAL_STEPS * 2):  # –ë–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∞
                try:
                    inputs, targets = next(val_iter)
                except StopIteration:
                    val_iter = iter(val_loader)
                    inputs, targets = next(val_iter)
                
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                _, predicted = torch.max(outputs, 1)
                
                all_predictions.extend(predicted.cpu().numpy())
                all_targets.extend(targets.cpu().numpy())
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏
        correct = sum([1 for p, t in zip(all_predictions, all_targets) if p == t])
        total = len(all_predictions)
        final_acc = 100 * correct / total if total > 0 else 0
        
        print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {final_acc:.2f}%")
        print(f"–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {total}")

if __name__ == "__main__":
    train_model()
