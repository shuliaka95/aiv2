# train.py
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'ml'))

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR
from tqdm import tqdm
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from dataset import get_dataloaders
from model import ModulationNet25M as MyCustomModel
from config import TRAIN_CONFIG, MODEL_CONFIG, NUM_CLASSES, NUM_IQ_SAMPLES

class VerboseReduceLROnPlateau(ReduceLROnPlateau):
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π scheduler —Å –≤—ã–≤–æ–¥–æ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."""
    def __init__(self, optimizer, mode='min', factor=0.1, patience=10,
                 threshold=1e-4, threshold_mode='rel', cooldown=0,
                 min_lr=0, eps=1e-8):
        super().__init__(optimizer, mode, factor, patience, threshold,
                         threshold_mode, cooldown, min_lr, eps)
    
    def step(self, metrics):
        old_lr = self.optimizer.param_groups[0]['lr']
        super().step(metrics)
        new_lr = self.optimizer.param_groups[0]['lr']
        
        if new_lr != old_lr:
            print(f"  ‚Üª Learning rate –∏–∑–º–µ–Ω–µ–Ω: {old_lr:.2e} ‚Üí {new_lr:.2e}")

def train_model():
    """–ú–æ—â–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è."""
    device = TRAIN_CONFIG['device']
    epochs = TRAIN_CONFIG['num_epochs']
    model_save_path = MODEL_CONFIG['model_path']
    
    print(f"=== –û–ë–£–ß–ï–ù–ò–ï –ú–û–©–ù–û–ô –ú–û–î–ï–õ–ò –ù–ê {NUM_CLASSES} –ö–õ–ê–°–°–û–í ===")
    print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    print(f"–≠–ø–æ—Ö: {epochs}")
    print(f"Batch size: {TRAIN_CONFIG['batch_size']}")
    print(f"Learning rate: {TRAIN_CONFIG['learning_rate']}")
    print("=" * 70)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    train_loader, val_loader = get_dataloaders(
        batch_size=TRAIN_CONFIG['batch_size'],
        num_iq_samples=NUM_IQ_SAMPLES
    )
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    test_batch = next(iter(train_loader))
    print(f"–§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö: {test_batch[0].shape}")
    print(f"–î–∏–∞–ø–∞–∑–æ–Ω –º–µ—Ç–æ–∫: [{test_batch[1].min().item()}, {test_batch[1].max().item()}]")
    print(f"–í—Å–µ –º–µ—Ç–∫–∏ < {NUM_CLASSES}? {test_batch[1].max().item() < NUM_CLASSES}")
    
    # –î–ª—è IterableDataset –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤
    STEPS_PER_EPOCH = 250  # –£–≤–µ–ª–∏—á–∏–ª –¥–ª—è –±–æ–ª—å—à–µ–π –º–æ–¥–µ–ª–∏
    VAL_STEPS = 75

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    model = MyCustomModel(num_classes=NUM_CLASSES).to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\n=== –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ú–û–î–ï–õ–ò ===")
    print(f"–í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
    print(f"–û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {trainable_params:,}")
    print(f"–ü—Ä–æ–ø–æ—Ä—Ü–∏—è –æ–±—É—á–∞–µ–º—ã—Ö: {trainable_params/total_params:.2%}")
    
    # –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å —Å label smoothing
    criterion = nn.CrossEntropyLoss(label_smoothing=0.15)  # –£–≤–µ–ª–∏—á–∏–ª smoothing
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å –±–æ–ª—å—à–∏–º weight decay
    optimizer = optim.AdamW(model.parameters(),
                          lr=TRAIN_CONFIG['learning_rate'],
                          weight_decay=2e-3,  # –£–≤–µ–ª–∏—á–∏–ª weight decay
                          betas=(0.9, 0.999))
    
    # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ schedulers
    scheduler_plateau = VerboseReduceLROnPlateau(optimizer, mode='max', factor=0.5,
                                                patience=5, min_lr=1e-7)  # –ë–æ–ª—å—à–µ patience
    scheduler_cosine = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-7)
    
    # –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
    best_val_acc = 0.0
    best_val_loss = float('inf')
    patience_counter = 0
    max_patience = 12  # –£–≤–µ–ª–∏—á–∏–ª patience
    
    train_losses, train_accs = [], []
    val_losses, val_accs = [], []
    
    # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    train_iter = iter(train_loader)
    
    for epoch in range(epochs):
        # === –û–ë–£–ß–ï–ù–ò–ï ===
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        progress_bar = tqdm(range(STEPS_PER_EPOCH),
                          desc=f"–≠–ø–æ—Ö–∞ {epoch+1}/{epochs}",
                          leave=False)
        
        for step in progress_bar:
            try:
                inputs, targets = next(train_iter)
            except StopIteration:
                train_iter = iter(train_loader)
                inputs, targets = next(train_iter)
            
            inputs, targets = inputs.to(device), targets.to(device)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –±–∞—Ç—á—É
            inputs = (inputs - inputs.mean(dim=(0, 2), keepdim=True)) / (inputs.std(dim=(0, 2), keepdim=True) + 1e-8)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            
            # –î–æ–±–∞–≤–ª—è–µ–º L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—é –≤—Ä—É—á–Ω—É—é
            l2_lambda = 0.002  # –£–≤–µ–ª–∏—á–∏–ª
            l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())
            loss = loss + l2_lambda * l2_norm
            
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.3)  # –£–º–µ–Ω—å—à–∏–ª
            
            optimizer.step()
            
            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            batch_total = targets.size(0)
            total += batch_total
            correct += (predicted == targets).sum().item()
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
            if step % 20 == 0:
                avg_loss = running_loss / (step + 1)
                accuracy = 100 * correct / total if total > 0 else 0
                progress_bar.set_postfix(
                    loss=f"{avg_loss:.4f}", 
                    acc=f"{accuracy:.2f}%",
                    lr=f"{optimizer.param_groups[0]['lr']:.1e}"
                )
        
        epoch_loss = running_loss / STEPS_PER_EPOCH
        epoch_acc = 100 * correct / total if total > 0 else 0
        train_losses.append(epoch_loss)
        train_accs.append(epoch_acc)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ cosine scheduler
        scheduler_cosine.step()
        
        # === –í–ê–õ–ò–î–ê–¶–ò–Ø ===
        model.eval()
        val_correct = 0
        val_total = 0
        val_running_loss = 0.0
        
        val_iter = iter(val_loader)
        with torch.no_grad():
            for step in range(VAL_STEPS):
                try:
                    inputs, targets = next(val_iter)
                except StopIteration:
                    val_iter = iter(val_loader)
                    inputs, targets = next(val_iter)
                
                inputs, targets = inputs.to(device), targets.to(device)
                
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                inputs = (inputs - inputs.mean(dim=(0, 2), keepdim=True)) / (inputs.std(dim=(0, 2), keepdim=True) + 1e-8)
                
                outputs = model(inputs)
                val_loss = criterion(outputs, targets)
                val_running_loss += val_loss.item()
                
                _, predicted = torch.max(outputs, 1)
                val_total += targets.size(0)
                val_correct += (predicted == targets).sum().item()
        
        val_loss = val_running_loss / VAL_STEPS
        val_acc = 100 * val_correct / val_total if val_total > 0 else 0
        val_losses.append(val_loss)
        val_accs.append(val_acc)
        
        # === –í–´–í–û–î –°–¢–ê–¢–ò–°–¢–ò–ö–ò ===
        print(f"\n{'='*70}")
        print(f"–≠–ø–æ—Ö–∞ {epoch+1:3d}/{epochs}:")
        print(f"  –û–±—É—á–µ–Ω–∏–µ  - –ü–æ—Ç–µ—Ä—è: {epoch_loss:.4f}, –¢–æ—á–Ω–æ—Å—Ç—å: {epoch_acc:6.2f}%")
        print(f"  –í–∞–ª–∏–¥–∞—Ü–∏—è - –ü–æ—Ç–µ—Ä—è: {val_loss:.4f}, –¢–æ—á–Ω–æ—Å—Ç—å: {val_acc:6.2f}%")
        print(f"  LR: {optimizer.param_groups[0]['lr']:.2e}")
        print(f"  Gap (train-val): {epoch_acc - val_acc:.1f}%")
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ plateau scheduler
        scheduler_plateau.step(val_acc)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
        overfitting_warning = ""
        
        if epoch >= 5:
            # –†–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É train –∏ val accuracy
            if epoch_acc - val_acc > 12:  # –£–º–µ–Ω—å—à–∏–ª –ø–æ—Ä–æ–≥
                overfitting_warning = f" ‚ö†Ô∏è  –†–∞–∑—Ä—ã–≤ train-val: {epoch_acc-val_acc:.1f}%"
            
            # –†–∞—Å—Ç—É—â–∞—è val loss
            if len(val_losses) >= 4 and all(val_losses[-i] > val_losses[-(i+1)] for i in range(1, 3)):
                overfitting_warning = " ‚ö†Ô∏è  Val loss —Ä–∞—Å—Ç–µ—Ç 2 —ç–ø–æ—Ö–∏ –ø–æ–¥—Ä—è–¥!"
            
            # –ü–∞–¥–∞—é—â–∞—è val accuracy
            if len(val_accs) >= 4 and all(val_accs[-i] < val_accs[-(i+1)] for i in range(1, 3)):
                overfitting_warning = " ‚ö†Ô∏è  Val accuracy –ø–∞–¥–∞–µ—Ç 2 —ç–ø–æ—Ö–∏ –ø–æ–¥—Ä—è–¥!"
        
        if overfitting_warning:
            print(f"  {overfitting_warning}")
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ dropout –ø—Ä–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–∏
            if hasattr(model, 'classifier'):
                for module in model.classifier:
                    if isinstance(module, nn.Dropout):
                        if module.p < 0.8:  # –ú–∞–∫—Å–∏–º—É–º 80%
                            module.p = min(0.8, module.p + 0.03)
                            print(f"  ‚Üª –£–≤–µ–ª–∏—á–µ–Ω Dropout: {module.p-0.03:.2f} ‚Üí {module.p:.2f}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_val_loss = val_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
                'val_loss': val_loss,
                'train_acc': epoch_acc,
                'train_loss': epoch_loss,
                'config': {
                    'num_classes': NUM_CLASSES,
                    'learning_rate': TRAIN_CONFIG['learning_rate'],
                    'batch_size': TRAIN_CONFIG['batch_size']
                }
            }, model_save_path)
            print(f"  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (—Ç–æ—á–Ω–æ—Å—Ç—å: {val_acc:.2f}%)")
            patience_counter = 0
        else:
            patience_counter += 1
            print(f"  ‚è≥ –ë–µ–∑ —É–ª—É—á—à–µ–Ω–∏–π: {patience_counter}/{max_patience}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
        if (epoch + 1) % 10 == 0:
            checkpoint_path = f"checkpoint_epoch_{epoch+1}.pth"
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_acc': epoch_acc,
                'val_acc': val_acc,
                'train_loss': epoch_loss,
                'val_loss': val_loss,
            }, checkpoint_path)
            print(f"  üíæ –ß–µ–∫–ø–æ–∏–Ω—Ç: {checkpoint_path}")
        
        # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
        if patience_counter >= max_patience:
            print(f"\n{'='*70}")
            print(f"‚ö†Ô∏è  –†–ê–ù–ù–Ø–Ø –û–°–¢–ê–ù–û–í–ö–ê –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
            print(f"   –¢–æ—á–Ω–æ—Å—Ç—å –Ω–µ —É–ª—É—á—à–∞–ª–∞—Å—å {max_patience} —ç–ø–æ—Ö")
            print(f"   –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_val_acc:.2f}%")
            break
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 20 —ç–ø–æ—Ö
        if (epoch + 1) % 20 == 0:
            print(f"\n  === –ü–†–û–ì–†–ï–°–° –ß–ï–†–ï–ó {epoch+1} –≠–ü–û–• ===")
            print(f"  Train accuracy: {train_accs[0]:.1f}% ‚Üí {epoch_acc:.1f}%")
            print(f"  Val accuracy: {val_accs[0]:.1f}% ‚Üí {val_acc:.1f}%")
            print(f"  –°—Ä–µ–¥–Ω–∏–π gap: {np.mean([t-v for t,v in zip(train_accs[-10:], val_accs[-10:])]):.1f}%")

    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\n{'='*70}")
    print(f"–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    print(f"–õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {best_val_acc:.2f}%")
    print(f"–õ—É—á—à–∞—è –ø–æ—Ç–µ—Ä—è –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {best_val_loss:.4f}")
    print(f"–í—Å–µ–≥–æ —ç–ø–æ—Ö: {len(train_accs)}")
    
    # –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    if len(train_accs) > 10:
        final_gap = train_accs[-1] - val_accs[-1]
        avg_gap = np.mean([t - v for t, v in zip(train_accs[-10:], val_accs[-10:])])
        
        print(f"\n=== –ê–ù–ê–õ–ò–ó –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–Ø ===")
        print(f"–§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ (train-val): {final_gap:.2f}%")
        print(f"–°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑—Ä—ã–≤ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —ç–ø–æ—Ö): {avg_gap:.2f}%")
        
        if avg_gap > 15:
            print("  ‚ö†Ô∏è  –°–ò–õ–¨–ù–û–ï –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï!")
            print("  –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –£–≤–µ–ª–∏—á—å—Ç–µ dropout, —É–º–µ–Ω—å—à–∏—Ç–µ –º–æ–¥–µ–ª—å")
        elif avg_gap > 8:
            print("  ‚ö†Ô∏è  –£–ú–ï–†–ï–ù–ù–û–ï –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï")
        else:
            print("  ‚úì –•–û–†–û–®–ê–Ø –û–ë–û–ë–©–ê–Æ–©–ê–Ø –°–ü–û–°–û–ë–ù–û–°–¢–¨")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∞
    if os.path.exists(model_save_path):
        checkpoint = torch.load(model_save_path)
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"\n–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å –∏–∑ —ç–ø–æ—Ö–∏ {checkpoint['epoch']+1}")
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ç–µ—Å—Ç
        model.eval()
        all_predictions = []
        all_targets = []
        
        val_iter = iter(val_loader)
        with torch.no_grad():
            for _ in range(VAL_STEPS * 2):  # –ë–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∞
                try:
                    inputs, targets = next(val_iter)
                except StopIteration:
                    val_iter = iter(val_loader)
                    inputs, targets = next(val_iter)
                
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                _, predicted = torch.max(outputs, 1)
                
                all_predictions.extend(predicted.cpu().numpy())
                all_targets.extend(targets.cpu().numpy())
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏
        correct = sum([1 for p, t in zip(all_predictions, all_targets) if p == t])
        total = len(all_predictions)
        final_acc = 100 * correct / total if total > 0 else 0
        
        print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {final_acc:.2f}%")
        print(f"–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {total}")

if __name__ == "__main__":
    train_model()